.. _qat_perf_opt_guide_design_guidelines:

Software Design Guidelines
==========================

This chapter focuses on key design decisions that should be considered,
to achieve optimal performance, when integrating with the Intel
QuickAssist Technology software. In many cases the best Intel
QuickAssist Technology configuration is dependent on the design of the
application stack that is being used and so it is not possible to have a
*one configuration fits all* approach. The trade-offs between differing
approaches will be discussed in this section to help the designer to
make an informed decision.

The guidelines presented here focus on the following performance
aspects:

- Maximizing throughput through the accelerator.
- Minimizing the offload cost incurred when using the accelerator.
- Minimizing latency.

Each guideline will highlight its impact on performance. Specific
performance numbers are not given in this document since exact
performance numbers depend on a variety of factors and tend to be
specific to a given workload, software, and platform configuration.

.. _qat_perf_opt_guide_polling_vs_interrupts:

Polling vs Interrupts
---------------------

.. note:: Not all use cases support interrupt mode, and not all software packages support interrupt mode.

Software can either periodically query the hardware accelerator for
responses or it can enable the generation of an interrupt when responses
are available. Interrupts or polling mode can be configured per instance
via the device configuration settings. Configuration parameter details
are available in the :doc:`Programmer's Guide </PG/index>`.

The properties and performance characteristics of each mode are
explained below followed by recommendations on selecting a
configuration.

Interrupt Mode 
++++++++++++++

When operating in interrupt mode, the accelerator will generate an MSI-X
interrupt when a response is placed on a response ring. Each ring bank
has a separate MSI-X interrupt which may be steered to a particular CPU
core via the ``CoreAffinity`` settings in the configuration file.

To reduce the number of interrupts generated, and hence the number of
CPU cycles spent processing interrupts, multiple responses can be
coalesced together. The presence of the multiple responses can be
indicated via a single coalesced interrupt rather than having an
interrupt per response. The number of responses that are associated with
a coalesced interrupt is determined by an interrupt coalescing timer.
When the accelerator places a response in a response ring, it starts an
interrupt coalescing timer. While the timer is running, additional
responses may be placed in the response ring. When the timer expires, an
interrupt is generated to indicate that responses are available. Details
on how to configure interrupt coalescing are available in the
:doc:`Programmer's Guide </PG/index>`.

Since interrupt coalescing is based on a timer, there is some
variability in the number of responses that are associated with an
interrupt. The arrival rate of responses is a function of the arrival
rate of the associated requests and of the request size. Hence, the
timer configuration needed to coalesce *X* large requests is different
from the timer configuration needed to coalesce *X* small requests. It is
recommended that the timer is tuned based on the average expected
request size.

The choice of timer configuration impacts throughput, latency, and
offload cost:

-  Configuring a very short timer period minimizes latency, but will
   increase the offload cost since there will be a higher number of
   interrupts and hence more CPU cycles spent processing the interrupts.
   If this interrupt processing becomes a performance bottleneck for the
   CPU, the overall system throughput will be impacted.

-  Configuring a very long timer period leads to reduced offload cost
   (due to the reduction in the number of interrupts) but increased
   latency. If the timer period is very long and causes the response
   rings to fill, the accelerator will stall and throughput will be
   impacted.

The appropriate coalescing timer configuration will depend on the
characteristics of the application. It is recommended that the value
chosen is tuned to achieve optimal performance.

Using interrupts to notify user space applications is achieved using
*epoll mode* which utilizes the kernel device drivers poll function to
allow an application to get notified of interrupt events.

Because epoll mode has two parts, of which the kernel space part
utilizes the interrupts, if there is a delay in the kernel interrupt
(such as, by changing the coalescing fields), there will be a
corresponding increase in latency in the delivery of the event to user
space.

The thread waiting for an event in epoll mode does not consume CPU time,
but the latency could have an impact on the performance. For higher
packet load where the wait time for the next packet is insignificant,
polling mode is recommended.

When using interrupts with the user space Intel QuickAssist Technology
driver, there is significant overhead in propagating the interrupt to
the user space process that the driver is running in. This leads to an
increased offload cost. Hence it is recommended that interrupts should
not be used with the user space Intel QuickAssist Technology driver.

.. _qat_perf_opt_guide_polling_mode:

Polling Mode
++++++++++++

In polled mode, interrupts are fully disabled, and the software
application must periodically invoke the polling API, provided by the
Intel QuickAssist Technology driver, to check for and process responses
from the hardware. Details of the polling API are available in the
:doc:`Programmer's Guide </PG/index>`.

The frequency of polling is a key performance parameter that should be
fine-tuned based on the application. This parameter has an impact on
throughput, latency and on offload cost:

-  If the polling frequency is too high, CPU cycles are wasted if there
   are no responses available when the polling routine is called. This
   leads to an increased offload cost.

-  If the polling frequency is too low, latency is increased, and
   throughput may be impacted if the response rings fill causing the
   accelerator to stall.

The choice of threading model has an impact on performance when using a
polling approach. There are two main threading approaches when polling:

-  Creating a polling thread that periodically calls the polling API.
   This model is often the simplest to implement, allows for maximum
   throughput, but can lead to increased offload cost due to the
   overhead associated with context switching to/from the polling
   thread.

-  Invoking the polling API and submitting new requests from within the
   same thread. This model is characterized by having a *dispatch loop*
   that alternates between submitting new requests and polling for
   responses. Additional steps are often included in the loop such as
   checking for received network packets or transmitting network
   packets. This approach often leads to the best performance since the
   polling rate can be easily tuned to match the submission rate so
   throughput is maximized and offload cost is minimized.

Recommendations
+++++++++++++++

Polling mode tends to be preferred in cases where traffic is steady
(such as packet processing applications) and can result in a minimal
offload cost. Epoll mode is preferred for cases where traffic is bursty,
as the application can sleep until there is a response to process.

Considerations when using polling mode:

-  Fine-tuning the polling interval is critical to achieving optimal
   performance.

-  The preference is for invoking the polling API and submitting new
   requests from within the same thread rather than having a separate
   polling thread.

Considerations when using epoll mode:

-  CPU usage will be at 0% in idle state in epoll mode versus a non-zero
   value in standard poll mode. However, with a high load state,
   standard poll mode should out-perform epoll mode.

Data Plane API vs Traditional API
--------------------------------------

The cryptographic and compression services provide two flavors of API,
known as the *Traditional API* and the *Data Plane API*. The traditional API
provides a full set of functionality including thread safety that allows
many application threads to submit operations to the same service
instance. The Data Plane API is aimed at reducing offload cost by
providing a *bare bones* API, with a set of constraints, which may suit
many applications. 

Refer to the `Intel QuickAssist Technology Cryptographic API Reference Manual <https://cdrdv2.intel.com/v1/dl/getContent/709199?explicitVersion=true>`_ for more details on the differences
between the Data Plane and traditional APIs for the crypto service.

From a throughput and latency perspective, there is no difference in
performance between the Data Plane API and the traditional API.

From an offload cost perspective, the Data Plane API uses significantly
fewer CPU cycles per request compared to the traditional API. For
example, the cryptographic Data Plane API has an offload cost that is
lower than the cryptographic traditional API.

Batch Submission of Requests Using the Data Plane API
+++++++++++++++++++++++++++++++++++++++++++++++++++++

The Data Plane API provides the capability to submit batches of requests
to the accelerator. The use of the batch mode of operation leads to a
reduction in the offload cost compared to submitting the requests one at
a time to the accelerator. This is due to CPU cycle savings arising from
fewer writes to the hardware ring registers in PCIe\* memory space.

Using the Data Plane API, batches of requests can be submitted to the
accelerator using either the ``cpaCySymDpEnqueueOp()`` or
``cpaCySymDpEnqueueOpBatch()`` API calls for the symmetric cryptographic
data plane API and using either the ``cpaDcDpEnqueueOp()`` or ``cpaDcDpEnqueueOpBatch()``
API calls for the compression data plane API. In
all cases, requests are only submitted to the accelerator when the
``performOpNow`` parameter is set to ``CPA_TRUE``.

It is recommended to use the batch submission mode of operation where
possible to reduce offload cost.

Synchronous (sync) vs Asynchronous (async)
------------------------------------------

The Intel QuickAssist Technology traditional API supports both
synchronous and asynchronous modes of operation. The Intel QuickAssist
Technology Data Plane API only supports the asynchronous mode of
operation.

With synchronous mode, the traditional API
will block and not return to the calling code until the acceleration
operation has completed.

With asynchronous mode, the traditional or Data Plane API will return to the calling code once the request has been
submitted to the accelerator. When the accelerator has completed the
operation, the completion is signaled via the invocation of a callback
function.

From a performance perspective, the accelerator requires multiple
inflight requests per acceleration engine to achieve maximum throughput.
With synchronous mode of operation, multiple threads must be used to
ensure that multiple requests are inflight. The use of multiple threads
introduces an overhead of context switching between the threads which
leads to an increase in offload cost.

Hence, the use of asynchronous mode is the recommended approach for
optimal performance.

Buffer Lists
------------

The symmetric cryptographic and compression Intel QuickAssist
Technology APIs use buffer lists for passing data to/from the hardware
accelerator. The number and size of elements in a buffer list has an
impact on throughput; performance degrades as the number of elements in
a buffer list increases. To minimize this degradation in throughput
performance, it is recommended to keep the number of buffers in a buffer
list to a minimum. Using a single buffer in a buffer list leads to
optimal performance. See also the section :ref:`Payload Alignment <qat_perf_opt_guide_payload_alignment>` for additional considerations.

.. note:: 

    Specific performance numbers are not given in this document since 
    exact performance numbers depend on a variety of factors and tend to
    be specific to a given workload, software and platform configuration.

When using the Data Plane API, it is possible to pass a flat buffer to
the API instead of a buffer list. This is the most efficient usage of
system resources (mainly PCIe\* bandwidth) and can lead to lower
latencies compared to using buffer lists.

In summary, the recommendations for using buffer lists are:

-  If using the Data Plane API, use a flat buffer instead of a buffer
   list.

-  If using a buffer list, a single buffer per buffer list leads to
   highest throughput performance.

-  If using a buffer list, keep the number of buffers in the list to a
   minimum.

Maximum Number of Concurrent Requests
-------------------------------------

The depth of the hardware rings used by the Intel QuickAssist
Technology driver for submitting requests to, and retrieving responses
from, the accelerator hardware can be controlled via the configuration
file using the ``CyXNumConcurrentSymRequests``, ``CyXNumConcurrentAsymRequests``
and ``DcXNumConcurrentRequests`` parameters. These settings can have an
impact on performance:

-  As the maximum number of concurrent requests is increased in the
   configuration file, the memory requirements required to support this
   also increases.

-  If the number of concurrent requests is set too low, there may not be
   enough outstanding requests to keep the accelerator busy and so
   throughput will degrade. The minimum number of concurrent requests
   required to keep the accelerator busy is a function of the size of
   the requests and of the rate at which responses are processed via
   either polling or interrupts (refer to the section :ref:`Polling vs Interrupts <qat_perf_opt_guide_polling_vs_interrupts>`
   for more details).

-  If the number of concurrent requests is set too high, the maximum
   latency will increase.

It is recommended that the maximum number of concurrent requests is
tuned to achieve the correct balance between memory usage, throughput
and latency for a given application. As a guide the maximum number
configured should reflect the peak request rate that the accelerator
must handle.

Symmetric Crypto Partial Operations
-----------------------------------

The symmetric cryptographic Intel QuickAssist Technology API supports
partial operations for some cryptographic algorithms. This allows a
single payload to be processed in multiple fragments with each fragment
corresponding to a partial operation. The Intel QuickAssist Technology
API implementation will maintain sufficient state between each partial
operation to allow a subsequent partial operation for the same session
to continue from where the previous operation finished.

From a performance perspective, the cost of maintaining the state and
the serialization between the partial requests in a session has a
negative impact on offload cost and throughput. To maximize performance
when using partial operations, multiple symmetric cryptographic sessions
must be used to ensure that sufficient requests are provided to the
hardware to keep it busy.

For optimal performance, it is recommended to avoid the use of partial
requests if possible.

There are some situations where the use of partials cannot be avoided
since the use of partials and the need to maintain state is inherent in
the higher level protocol (such as, the use of the RC4 cipher with an
SSL/TLS protocol stack).

Reusing Sessions
----------------

The session is the entry point to perform symmetric cryptography with
the Intel QAT device. Every session has assigned algorithm, state, instance,
but also allocated memory space.

If you are limited with the number of instances and want to run several
different algorithms or change keys for another session, de-initialize
the session and create a new one. However, such an approach impacts
performance because it involves buffer disposal, deinitialization of the
instance, and so on.

Instead, the session can be reused with updating only a direction
(encryption/decryption), key or symmetric algorithm to be used. This
method will not dispose buffers and can reduce the CPU cycles
significantly.

Maximizing Intel QAT Device Utilization
---------------------------------------

The Intel QAT device utilization and throughput are maximized
when there are sufficient requests outstanding to occupy the multiple
internal acceleration engines with the device.

It also recommended to assign each Intel QAT service instance
to a separate CPU core to balance the load across the CPU and to ensure
that there are sufficient CPU cycles to drive the accelerators at
maximum performance.

When using interrupts, the core affinity settings within the
configuration file should be used to steer the interrupts for a service
instance to the appropriate core.

Detailed guidelines on load balancing and how to ensure maximum use of
the available hardware capacity are available in the :doc:`Programmer's Guide </PG/index>`.

Best Known Method (BKM) for Avoiding Performance Bottlenecks
------------------------------------------------------------

For optimal performance, ensure the following:

-  All data buffers should be aligned on a 64-byte boundary.

-  Transfer sizes that are multiples of 64 bytes are optimal.

-  Small data transfers (less than 64 bytes) should be avoided. If a
   small data transfer is needed, consider embedding this within a
   larger buffer so that the transfer size is a multiple of 64 bytes.
   Offsets can then be used to identify the region of interest within
   the larger buffer.

-  Each buffer entry within a Scatter-Gather-List (SGL) should be a
   multiple of 64bytes and should be aligned on a 64-byte boundary.

Avoid Data Copies By Using SVM & ATS
------------------------------------

.. note:: This applies only to platforms starting with Intel QAT Gen 4.

On CPUs and Intel QAT devices that support Shared Virtual
Memory (SVM), virtual addresses to virtually contiguous buffers can be
supplied to the Intel QAT hardware. Without this support, physical
addresses to physically contiguous and DMAable memory buffers must be
used. Using virtual addressed memory avoids the need to copy payload
data from user space memory allocated with ``malloc()`` to physically
contiguous memory.

When SVM is enabled, the Intel QAT device interacts with the
IOMMU to fetch the virtual to physical address translations when
accessing memory and this can result in increased latency and lower
throughput.

Avoid Page Faults When Using SVM
++++++++++++++++++++++++++++++++

When using SVM to avoid data copies, there is a chance that after a
request, that refers to a virtually addressed buffer, has been submitted
to the Intel QAT device, the operating system may swap out the
memory pages associated with that buffer. This will result in a page
fault when the Intel QAT device tries to access the memory. The Intel
QAT device will stall the processing of that request until the page
fault is resolved or times out. This can lead to an underutilization of
the Intel QAT device. To avoid page faults, the memory submitted to Intel QAT
should be pinned.